# app.py
# -*- coding: utf-8 -*-
"""
AGENTE COM PLANEJAMENTO DIN√ÇMICO - VERS√ÉO STREAMLIT COMPLETA
Mant√©m a l√≥gica robusta do c√≥digo original com interface Web
"""

import streamlit as st
import json
import time
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import requests
import glob
import os
import re
import unicodedata

# --- CONFIGURA√á√ÉO DA P√ÅGINA STREAMLIT ---
st.set_page_config(
    page_title="üöÄ Agente de An√°lise LTIP",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- CONFIGURA√á√ïES E ESTRUTURAS DE CONHECIMENTO (DO C√ìDIGO ORIGINAL) ---
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
TOP_K_SEARCH = 7
AMBIGUITY_THRESHOLD = 3

# Dicion√°rio especializado para termos t√©cnicos de LTIP (MANTIDO ORIGINAL)
TERMOS_TECNICOS_LTIP = {
    "tratamento de dividendos": [
        "tratamento de dividendos", "equivalente em dividendos", "dividendos", 
        "juros sobre capital pr√≥prio", "proventos", "dividend equivalent",
        "dividendos pagos em a√ß√µes", "ajustes por dividendos"
    ],
    "pre√ßo de exerc√≠cio": [
        "pre√ßo de exerc√≠cio", "strike price", "pre√ßo de compra", "pre√ßo fixo", 
        "valor de exerc√≠cio", "pre√ßo pr√©-estabelecido", "pre√ßo de aquisi√ß√£o"
    ],
    "forma de liquida√ß√£o": [
        "forma de liquida√ß√£o", "liquida√ß√£o", "pagamento", "entrega f√≠sica", 
        "pagamento em dinheiro", "transfer√™ncia de a√ß√µes", "settlement"
    ],
    "vesting": [
        "vesting", "per√≠odo de car√™ncia", "car√™ncia", "aquisi√ß√£o de direitos", 
        "cronograma de vesting", "vesting schedule", "per√≠odo de cliff"
    ],
    "eventos corporativos": [
        "eventos corporativos", "desdobramento", "grupamento", "dividendos pagos em a√ß√µes",
        "bonifica√ß√£o", "split", "ajustes", "reorganiza√ß√£o societ√°ria"
    ],
    "stock options": [
        "stock options", "op√ß√µes de a√ß√µes", "op√ß√µes de compra", "SOP", 
        "plano de op√ß√µes", "ESOP", "op√ß√£o de compra de a√ß√µes"
    ],
    "a√ß√µes restritas": [
        "a√ß√µes restritas", "restricted shares", "RSU", "restricted stock units", 
        "a√ß√µes com restri√ß√£o", "plano de a√ß√µes restritas"
    ]
}

# T√≥picos expandidos de an√°lise (MANTIDO ORIGINAL)
AVAILABLE_TOPICS = [
    "termos e condi√ß√µes gerais", "data de aprova√ß√£o e √≥rg√£o respons√°vel",
    "n√∫mero m√°ximo de a√ß√µes abrangidas", "n√∫mero m√°ximo de op√ß√µes a serem outorgadas",
    "condi√ß√µes de aquisi√ß√£o de a√ß√µes", "crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio",
    "pre√ßo de exerc√≠cio", "strike price",
    "crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio", 
    "forma de liquida√ß√£o", "liquida√ß√£o", "pagamento",
    "restri√ß√µes √† transfer√™ncia das a√ß√µes", "crit√©rios e eventos de suspens√£o/extin√ß√£o",
    "efeitos da sa√≠da do administrador", "Tipos de Planos", "Condi√ß√µes de Car√™ncia", 
    "Vesting", "per√≠odo de car√™ncia", "cronograma de vesting",
    "Matching", "contrapartida", "co-investimento",
    "Lockup", "per√≠odo de lockup", "restri√ß√£o de venda",
    "Tratamento de Dividendos", "equivalente em dividendos", "proventos",
    "Stock Options", "op√ß√µes de a√ß√µes", "SOP",
    "A√ß√µes Restritas", "RSU", "restricted shares",
    "Eventos Corporativos", "IPO", "grupamento", "desdobramento"
]

# --- FUN√á√ÉO SEGURA PARA CHAMADAS DE API (CONSERVANDO PROTE√á√ÉO) ---
def safe_api_call(url, payload, headers, timeout=90):
    """Fun√ß√£o segura para chamadas de API sem expor a chave em erros."""
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=timeout)
        response.raise_for_status()
        return response.json(), None
    except requests.exceptions.HTTPError as e:
        status_code = e.response.status_code
        reason = e.response.reason
        return None, f"Erro de API com c√≥digo {status_code}: {reason}. Por favor, tente novamente mais tarde."
    except requests.exceptions.RequestException:
        return None, "Erro de conex√£o ao tentar contatar a API. Verifique sua conex√£o com a internet."

# --- FUN√á√ïES PRINCIPAIS (MANTIDAS DO C√ìDIGO ORIGINAL) ---

def expand_search_terms(base_term):
    """Expande um termo base com sin√¥nimos e varia√ß√µes t√©cnicas."""
    expanded_terms = [base_term.lower()]
    
    for category, terms in TERMOS_TECNICOS_LTIP.items():
        if any(term.lower() in base_term.lower() for term in terms):
            expanded_terms.extend([term.lower() for term in terms])
    
    return list(set(expanded_terms))

def search_by_tags(artifacts, company_name, target_tags):
    """Busca chunks que contenham tags espec√≠ficas para uma empresa. (MANTIDO ORIGINAL)"""
    results = []
    
    for index_name, artifact_data in artifacts.items():
        chunk_data = artifact_data['chunks']
        
        for i, mapping in enumerate(chunk_data.get('map', [])):
            document_path = mapping['document_path']
            if re.search(re.escape(company_name.split(' ')[0]), document_path, re.IGNORECASE):
                chunk_text = chunk_data["chunks"][i]
                
                # Verifica se o chunk cont√©m as tags procuradas
                for tag in target_tags:
                    if f"T√≥picos:" in chunk_text and tag in chunk_text:
                        results.append({
                            'text': chunk_text,
                            'path': document_path,
                            'index': i,
                            'source': index_name,
                            'tag_found': tag
                        })
                        st.write(f"     -> Encontrado chunk com tag '{tag}' em {document_path}")
                        break
    
    return results

@st.cache_resource
def load_all_artifacts():
    """Carrega todos os artefatos e constr√≥i um cat√°logo de nomes de empresas can√¥nicos. (CONSERVANDO L√ìGICA ORIGINAL)"""
    artifacts = {}
    canonical_company_names = set()
    
    # CONSERVA O ACESSO AOS ARQUIVOS FAISS
    google_drive_path = st.session_state.get('google_drive_path', './data')
    
    if not os.path.exists(google_drive_path):
        st.error(f"ERRO CR√çTICO: Pasta n√£o encontrada: {google_drive_path}")
        st.info("Por favor, configure o caminho correto dos arquivos FAISS na barra lateral.")
        return None, None, None
    
    st.info("--- Carregando m√∫ltiplos artefatos ---")
    
    with st.spinner(f"Carregando modelo de embedding '{MODEL_NAME}'..."):
        model = SentenceTransformer(MODEL_NAME)

    index_files = glob.glob(os.path.join(google_drive_path, '*_faiss_index.bin'))
    if not index_files:
        st.error(f"ERRO CR√çTICO: Nenhum arquivo de √≠ndice (*_faiss_index.bin) encontrado em '{google_drive_path}'.")
        st.info("Certifique-se de que os arquivos FAISS est√£o na pasta correta.")
        return None, None, None

    progress_bar = st.progress(0)
    total_files = len(index_files)
    
    for idx, index_file in enumerate(index_files):
        category = os.path.basename(index_file).replace('_faiss_index.bin', '')
        chunks_file = os.path.join(google_drive_path, f"{category}_chunks_map.json")
        
        try:
            st.info(f"Carregando categoria de documento '{category}'...")
            index = faiss.read_index(index_file)
            
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            
            artifacts[category] = {'index': index, 'chunks': chunk_data}
            
            # Extrai nomes das empresas
            for mapping in chunk_data.get('map', []):
                company_name = mapping['document_path'].split('/')[0]
                canonical_company_names.add(company_name)
            
            progress_bar.progress((idx + 1) / total_files)
            
        except FileNotFoundError:
            st.warning(f"AVISO: Arquivo de chunks '{chunks_file}' n√£o encontrado para o √≠ndice '{index_file}'. Pulando.")
            continue
        except Exception as e:
            st.error(f"Erro ao carregar '{category}': {e}")
            continue

    if not artifacts:
        st.error("ERRO CR√çTICO: Nenhum artefato foi carregado com sucesso.")
        return None, None, None

    st.success(f"--- {len(artifacts)} categorias de documentos carregadas com sucesso! ---")
    st.success(f"--- {len(canonical_company_names)} empresas √∫nicas identificadas. ---")
    
    return artifacts, model, list(canonical_company_names)

def create_dynamic_analysis_plan(query, company_catalog, available_indices):
    """CHAMADA AO LLM PLANEJADOR: Gera um plano de a√ß√£o din√¢mico em JSON - CONSERVANDO L√ìGICA ORIGINAL."""
    
    # CONSERVA O ACESSO SEGURO √Ä API KEY
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
    except:
        api_key = st.session_state.get('gemini_api_key')
        if not api_key:
            st.error("ERRO: API Key do Gemini n√£o configurada!")
            return {"status": "error", "message": "API Key n√£o encontrada"}
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
    
    # Fun√ß√£o de normaliza√ß√£o melhorada (MANTIDA ORIGINAL)
    def normalize_name(name):
        try:
            nfkd_form = unicodedata.normalize('NFKD', name.lower())
            name = "".join([c for c in nfkd_form if not unicodedata.combining(c)])
            name = re.sub(r'[.,-]', '', name)
            suffixes = [r'\bs\.?a\.?\b', r'\bltda\b', r'\bholding\b', r'\bparticipacoes\b', r'\bcia\b', r'\bind\b', r'\bcom\b']
            for suffix in suffixes:
                name = re.sub(suffix, '', name, flags=re.IGNORECASE)
            return re.sub(r'\s+', '', name).strip()
        except Exception as e:
            st.error(f"Erro na normaliza√ß√£o: {e}")
            return name.lower()

    # IDENTIFICA√á√ÉO ROBUSTA DE EMPRESAS COM BUSCA HIER√ÅRQUICA (MANTIDA ORIGINAL)
    mentioned_companies = []
    query_clean = query.lower().strip()
    
    st.write(f"   -> Buscando empresas na query: '{query_clean}'")
    
    for canonical_name in company_catalog:
        found = False
        
        # 1. BUSCA EXATA
        if canonical_name.lower() == query_clean:
            mentioned_companies.append(canonical_name)
            st.write(f"   -> Correspond√™ncia EXATA: {canonical_name}")
            found = True
            continue
        
        # 2. BUSCA POR SUBSTRING (nome completo na query)
        if canonical_name.lower() in query_clean:
            mentioned_companies.append(canonical_name)
            st.write(f"   -> Correspond√™ncia por SUBSTRING: {canonical_name}")
            found = True
            continue
        
        # 3. BUSCA POR PARTES DO NOME
        company_parts = canonical_name.split(' ')
        for part in company_parts:
            if len(part) > 2 and re.search(r'\b' + re.escape(part.lower()) + r'\b', query_clean):
                if canonical_name not in mentioned_companies:
                    mentioned_companies.append(canonical_name)
                    st.write(f"   -> Correspond√™ncia por PARTE: {canonical_name} (parte: {part})")
                    found = True
                break
        
        if found:
            continue
        
        # 4. BUSCA SIMPLIFICADA (√∫ltimo recurso)
        normalized_canonical = normalize_name(canonical_name)
        normalized_query = normalize_name(query_clean)
        
        if normalized_query and len(normalized_query) > 2:
            if normalized_query in normalized_canonical:
                mentioned_companies.append(canonical_name)
                st.write(f"   -> Correspond√™ncia SIMPLIFICADA: {canonical_name}")
    
    st.write(f"   -> Empresas identificadas: {mentioned_companies}")
    
    # Se n√£o encontrou empresas, tenta busca mais agressiva
    if not mentioned_companies:
        st.write("   -> Tentando busca mais agressiva...")
        for canonical_name in company_catalog:
            # Busca por siglas (ex: CCR, Vibra)
            if len(query_clean) <= 6:  # Prov√°vel sigla ou nome curto
                if query_clean.upper() in canonical_name.upper():
                    mentioned_companies.append(canonical_name)
                    st.write(f"   -> Correspond√™ncia por SIGLA: {canonical_name}")

    prompt = f"""
Voc√™ √© um planejador de an√°lise. Sua tarefa √© analisar a "Pergunta do Usu√°rio" e identificar os t√≥picos de interesse.

**Instru√ß√µes:**
1. **Identifique os T√≥picos:** Analise a pergunta para identificar os t√≥picos de interesse. Se a pergunta for gen√©rica (ex: "resumo dos planos", "an√°lise da empresa"), inclua todos os "T√≥picos de An√°lise Dispon√≠veis". Se for espec√≠fica (ex: "fale sobre o vesting e dividendos"), inclua apenas os t√≥picos relevantes.
2. **Formate a Sa√≠da:** Retorne APENAS uma lista JSON de strings contendo os t√≥picos identificados.

**T√≥picos de An√°lise Dispon√≠veis:** {json.dumps(AVAILABLE_TOPICS, indent=2)}

**Pergunta do Usu√°rio:** "{query}"

**T√≥picos de Interesse (responda APENAS com a lista JSON de strings):**
"""
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    
    # USA A FUN√á√ÉO SEGURA PARA CHAMADAS DE API
    response_data, error_message = safe_api_call(url, payload, headers, timeout=90)
    
    if error_message:
        st.warning(f"Erro no planejamento: {error_message}")
        # Fallback em caso de erro
        plan = {"empresas": mentioned_companies, "topicos": AVAILABLE_TOPICS}
        return {"status": "success", "plan": plan}
    
    try:
        text_response = response_data['candidates'][0]['content']['parts'][0]['text']
        json_match = re.search(r'\[.*\]', text_response, re.DOTALL)
        if json_match:
            topics = json.loads(json_match.group(0))
            plan = {"empresas": mentioned_companies, "topicos": topics}
            return {"status": "success", "plan": plan}
        else:
            # Fallback: usa todos os t√≥picos se n√£o conseguir extrair JSON
            plan = {"empresas": mentioned_companies, "topicos": AVAILABLE_TOPICS}
            return {"status": "success", "plan": plan}
    except Exception as e:
        st.error(f"Erro ao processar resposta da IA: {e}")
        # Fallback em caso de erro
        plan = {"empresas": mentioned_companies, "topicos": AVAILABLE_TOPICS}
        return {"status": "success", "plan": plan}

def execute_dynamic_plan(plan, query_intent, artifacts, model):
    """EXECUTOR APRIMORADO: Busca exaustiva no item 8.4 + busca por tags + expans√£o de termos. (MANTIDO ORIGINAL)"""
    full_context = ""
    all_retrieved_docs = set()
    
    if query_intent == 'item_8_4_query':
        st.write("-> Estrat√©gia: BUSCA EXAUSTIVA ITEM 8.4 + COMPLEMENTO")
        
        for empresa in plan.get("empresas", []):
            full_context += f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
            
            # FASE 1: BUSCA EXAUSTIVA NO ITEM 8.4
            if 'item_8_4' in artifacts:
                st.write(f"   -> FASE 1: Recupera√ß√£o EXAUSTIVA do Item 8.4 para {empresa}")
                
                artifact_data = artifacts['item_8_4']
                chunk_data = artifact_data['chunks']
                
                # Recupera TODOS os chunks da empresa no item 8.4
                empresa_chunks_8_4 = []
                for i, mapping in enumerate(chunk_data.get('map', [])):
                    document_path = mapping['document_path']
                    if re.search(re.escape(empresa.split(' ')[0]), document_path, re.IGNORECASE):
                        chunk_text = chunk_data["chunks"][i]
                        all_retrieved_docs.add(str(document_path))
                        empresa_chunks_8_4.append({
                            'text': chunk_text,
                            'path': document_path,
                            'index': i
                        })
                
                st.write(f"     -> Encontrados {len(empresa_chunks_8_4)} chunks do Item 8.4")
                
                # Adiciona TODOS os chunks do item 8.4 ao contexto
                full_context += f"=== SE√á√ÉO COMPLETA DO ITEM 8.4 - {empresa.upper()} ===\n\n"
                for chunk_info in empresa_chunks_8_4:
                    full_context += f"--- Chunk Item 8.4 (Doc: {chunk_info['path']}) ---\n"
                    full_context += f"{chunk_info['text']}\n\n"
                
                full_context += f"=== FIM DA SE√á√ÉO ITEM 8.4 - {empresa.upper()} ===\n\n"
            
            # FASE 2: BUSCA COMPLEMENTAR COM EXPANS√ÉO DE TERMOS (limitada)
            st.write(f"   -> FASE 2: Busca complementar com expans√£o de termos para {empresa}")
            
            complementary_indices = [idx for idx in artifacts.keys() if idx != 'item_8_4']
            
            for topico in plan.get("topicos", [])[:10]:  # Limita a 10 t√≥picos
                expanded_terms = expand_search_terms(topico)
                
                st.write(f"     -> Buscando complemento para '{topico}'...")
                
                for term in expanded_terms[:5]:  # Limita a 5 termos
                    search_query = f"item 8.4 {term} empresa {empresa}"
                    
                    for index_name in complementary_indices:
                        if index_name in artifacts:
                            artifact_data = artifacts[index_name]
                            index = artifact_data['index']
                            chunk_data = artifact_data['chunks']
                            
                            query_embedding = model.encode([search_query], normalize_embeddings=True).astype('float32')
                            scores, indices = index.search(query_embedding, 3)
                            
                            chunks_found = 0
                            for i, idx in enumerate(indices[0]):
                                if idx != -1 and idx < len(chunk_data.get("chunks", [])) and scores[0][i] > 0.5:
                                    document_path = chunk_data["map"][idx]['document_path']
                                    if re.search(re.escape(empresa.split(' ')[0]), document_path, re.IGNORECASE):
                                        chunk_text = chunk_data["chunks"][idx]
                                        
                                        chunk_hash = hash(chunk_text[:100])
                                        if chunk_hash not in all_retrieved_docs:
                                            all_retrieved_docs.add(chunk_hash)
                                            score = scores[0][i]
                                            full_context += f"--- Contexto COMPLEMENTAR para '{topico}' via '{term}' (Fonte: {index_name}, Score: {score:.3f}) ---\n{chunk_text}\n\n"
                                            chunks_found += 1
                            
                            if chunks_found > 0:
                                st.write(f"         -> {chunks_found} chunks complementares de '{index_name}' para '{term}'")
                                break
                    
                    if chunks_found > 0:
                        break
            
            full_context += f"--- FIM DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
    
    else:
        # BUSCA GERAL COM TAGS E EXPANS√ÉO DE TERMOS
        st.write("-> Estrat√©gia: BUSCA GERAL COM TAGS E EXPANS√ÉO DE TERMOS")
        
        for empresa in plan.get("empresas", []):
            full_context += f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
            
            # FASE 1: BUSCA POR TAGS ESPEC√çFICAS
            st.write(f"   -> FASE 1: Busca por tags espec√≠ficas para {empresa}")
            
            target_tags = []
            for topico in plan.get("topicos", []):
                expanded_terms = expand_search_terms(topico)
                target_tags.extend(expanded_terms)
            
            # Remove duplicatas e mant√©m apenas termos relevantes
            target_tags = list(set([tag.title() for tag in target_tags if len(tag) > 3]))
            
            st.write(f"     -> Tags procuradas: {target_tags[:5]}...")
            
            tagged_chunks = search_by_tags(artifacts, empresa, target_tags)
            
            if tagged_chunks:
                full_context += f"=== CHUNKS COM TAGS ESPEC√çFICAS - {empresa.upper()} ===\n\n"
                for chunk_info in tagged_chunks:
                    full_context += f"--- Chunk com tag '{chunk_info['tag_found']}' (Doc: {chunk_info['path']}) ---\n"
                    full_context += f"{chunk_info['text']}\n\n"
                    all_retrieved_docs.add(str(chunk_info['path']))
                full_context += f"=== FIM DOS CHUNKS COM TAGS - {empresa.upper()} ===\n\n"
            
            # FASE 2: BUSCA SEM√ÇNTICA COMPLEMENTAR
            st.write(f"   -> FASE 2: Busca sem√¢ntica complementar para {empresa}")
            
            indices_to_search = list(artifacts.keys())
            
            for topico in plan.get("topicos", []):
                expanded_terms = expand_search_terms(topico)
                
                st.write(f"     -> Buscando '{topico}'...")
                
                for term in expanded_terms[:3]:  # Top 3 termos
                    search_query = f"informa√ß√µes sobre {term} no plano de remunera√ß√£o da empresa {empresa}"
                    
                    chunks_found = 0
                    for index_name in indices_to_search:
                        if index_name in artifacts:
                            artifact_data = artifacts[index_name]
                            index = artifact_data['index']
                            chunk_data = artifact_data['chunks']
                            
                            query_embedding = model.encode([search_query], normalize_embeddings=True).astype('float32')
                            scores, indices = index.search(query_embedding, TOP_K_SEARCH)
                            
                            for i, idx in enumerate(indices[0]):
                                if idx != -1 and scores[0][i] > 0.4:
                                    document_path = chunk_data["map"][idx]['document_path']
                                    if re.search(re.escape(empresa.split(' ')[0]), document_path, re.IGNORECASE):
                                        chunk_text = chunk_data["chunks"][idx]
                                        
                                        chunk_hash = hash(chunk_text[:100])
                                        if chunk_hash not in all_retrieved_docs:
                                            all_retrieved_docs.add(chunk_hash)
                                            score = scores[0][i]
                                            full_context += f"--- Contexto para '{topico}' via '{term}' (Fonte: {index_name}, Score: {score:.3f}) ---\n{chunk_text}\n\n"
                                            chunks_found += 1
                    
                    if chunks_found > 0:
                        st.write(f"       -> {chunks_found} chunks encontrados para '{term}'")
                        break
            
            full_context += f"--- FIM DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
    
    # CORRE√á√ÉO: Converte todos os elementos para string
    return full_context, [str(doc) for doc in all_retrieved_docs]

def get_final_unified_answer(query, context):
    """S√çNTESE APRIMORADA: Processa contexto exaustivo do item 8.4. (CONSERVANDO L√ìGICA ORIGINAL)"""
    
    # CONSERVA O ACESSO SEGURO √Ä API KEY
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
    except:
        api_key = st.session_state.get('gemini_api_key')
        if not api_key:
            return "ERRO: API Key do Gemini n√£o configurada para gerar resposta final!"
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
    
    # Detecta se h√° se√ß√£o completa do item 8.4
    has_complete_8_4 = "=== SE√á√ÉO COMPLETA DO ITEM 8.4" in context
    has_tagged_chunks = "=== CHUNKS COM TAGS ESPEC√çFICAS" in context
    
    if has_complete_8_4:
        structure_instruction = """
**ESTRUTURA OBRIGAT√ìRIA PARA ITEM 8.4:**
Use a estrutura oficial do item 8.4 do Formul√°rio de Refer√™ncia:
a) Termos e condi√ß√µes gerais dos planos
b) Data de aprova√ß√£o e √≥rg√£o respons√°vel
c) N√∫mero m√°ximo de a√ß√µes abrangidas pelos planos
d) N√∫mero m√°ximo de op√ß√µes a serem outorgadas
e) Condi√ß√µes de aquisi√ß√£o de a√ß√µes
f) Crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio
g) Crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio
h) Forma de liquida√ß√£o
i) Restri√ß√µes √† transfer√™ncia das a√ß√µes
j) Crit√©rios e eventos que, quando verificados, ocasionar√£o a suspens√£o, altera√ß√£o ou extin√ß√£o do plano
k) Efeitos da sa√≠da do administrador do cargo na manuten√ß√£o dos seus direitos no plano

Para cada subitem, extraia e organize as informa√ß√µes encontradas na SE√á√ÉO COMPLETA DO ITEM 8.4.
"""
    elif has_tagged_chunks:
        structure_instruction = "**PRIORIZE** as informa√ß√µes dos CHUNKS COM TAGS ESPEC√çFICAS e organize a resposta de forma l√≥gica usando Markdown."
    else:
        structure_instruction = "Organize a resposta de forma l√≥gica e clara usando Markdown."
    
    prompt = f"""
Voc√™ √© um analista financeiro s√™nior especializado em Formul√°rios de Refer√™ncia da CVM. 

**PERGUNTA ORIGINAL DO USU√ÅRIO:**
"{query}"

**CONTEXTO COLETADO DOS DOCUMENTOS:**
{context}

{structure_instruction}

**INSTRU√á√ïES PARA O RELAT√ìRIO FINAL:**
1. Responda diretamente √† pergunta do usu√°rio
2. **PRIORIZE** as informa√ß√µes da SE√á√ÉO COMPLETA DO ITEM 8.4 quando dispon√≠vel
3. **PRIORIZE** as informa√ß√µes dos CHUNKS COM TAGS ESPEC√çFICAS quando dispon√≠vel
4. Use informa√ß√µes complementares apenas para esclarecer ou expandir pontos espec√≠ficos
5. Seja detalhado, preciso e profissional
6. Transcreva dados importantes como valores, datas e percentuais
7. Se alguma informa√ß√£o n√£o estiver dispon√≠vel, indique: "Informa√ß√£o n√£o encontrada nas fontes analisadas"
8. Mantenha a estrutura t√©cnica apropriada para administradores de LTIP

**RELAT√ìRIO ANAL√çTICO FINAL:**
"""
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}
    }
    headers = {'Content-Type': 'application/json'}
    
    # USA A FUN√á√ÉO SEGURA PARA CHAMADAS DE API
    response_data, error_message = safe_api_call(url, payload, headers, timeout=180)
    
    if error_message:
        return f"ERRO ao gerar resposta final: {error_message}"
    
    try:
        return response_data['candidates'][0]['content']['parts'][0]['text'].strip()
    except Exception as e:
        return f"ERRO ao processar resposta final: {e}"

# --- INTERFACE STREAMLIT ---

def main():
    st.title("üöÄ Agente com Planejamento Din√¢mico - LTIP")
    st.markdown("**An√°lise inteligente de Formul√°rios de Refer√™ncia da CVM**")
    
    # --- SIDEBAR - CONFIGURA√á√ïES ---
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√µes")
        
        # CONSERVA O ACESSO √Ä API KEY
        st.subheader("üîë API Key do Gemini")
        gemini_api_key = st.text_input(
            "Insira sua API Key do Google Gemini",
            type="password",
            help="Sua chave ser√° mantida segura e n√£o ser√° exposta em erros."
        )
        if gemini_api_key:
            st.session_state['gemini_api_key'] = gemini_api_key
            st.success("‚úÖ API Key configurada com seguran√ßa!")
        
        st.divider()
        
        # CONSERVA O ACESSO AOS ARQUIVOS FAISS
        st.subheader("üìÅ Arquivos FAISS")
        google_drive_path = st.text_input(
            "Caminho dos arquivos FAISS",
            value="./data",
            help="Pasta onde est√£o os arquivos *_faiss_index.bin e *_chunks_map.json"
        )
        if google_drive_path:
            st.session_state['google_drive_path'] = google_drive_path
            
        if os.path.exists(google_drive_path):
            faiss_files = glob.glob(os.path.join(google_drive_path, '*_faiss_index.bin'))
            st.info(f"‚úÖ {len(faiss_files)} arquivos FAISS encontrados")
        else:
            st.error(f"‚ùå Pasta n√£o encontrada: {google_drive_path}")
        
        st.divider()
        
        # Bot√£o para recarregar
        if st.button("üîÑ Recarregar Artefatos", type="primary"):
            # Limpa cache
            if 'loaded_artifacts' in st.session_state:
                del st.session_state['loaded_artifacts']
            if 'embedding_model' in st.session_state:
                del st.session_state['embedding_model']
            if 'company_catalog' in st.session_state:
                del st.session_state['company_catalog']
            st.rerun()

    # --- CARREGAMENTO DOS ARTEFATOS ---
    if 'loaded_artifacts' not in st.session_state:
        if not st.session_state.get('gemini_api_key') and 'GEMINI_API_KEY' not in st.secrets:
            st.warning("‚ö†Ô∏è Por favor, configure a API Key do Gemini na barra lateral ou nos secrets do Streamlit.")
            return
            
        with st.spinner("Carregando artefatos pela primeira vez..."):
            artifacts, model, company_catalog = load_all_artifacts()
        
        if artifacts is None:
            st.error("‚ùå Falha no carregamento dos artefatos. Verifique as configura√ß√µes.")
            return
        
        st.session_state['loaded_artifacts'] = artifacts
        st.session_state['embedding_model'] = model
        st.session_state['company_catalog'] = company_catalog

    # --- INTERFACE PRINCIPAL ---
    loaded_artifacts = st.session_state['loaded_artifacts']
    embedding_model = st.session_state['embedding_model']
    company_catalog = st.session_state['company_catalog']
    
    # Exibe informa√ß√µes dos artefatos carregados
    col1, col2, col3 = st.columns(3)
    with col1:
        st.metric("üìä Categorias de Documentos", len(loaded_artifacts))
    with col2:
        st.metric("üè¢ Empresas Identificadas", len(company_catalog))
    with col3:
        st.metric("ü§ñ Modelo Embedding", MODEL_NAME.split('/')[-1])
    
    with st.expander("üìã Ver Empresas no Cat√°logo"):
        st.write(sorted(company_catalog))
    
    # Exemplos de perguntas
    st.subheader("üí° Exemplos de Perguntas")
    
    col1, col2, col3 = st.columns(3)
    with col1:
        if st.button("üìÑ Item 8.4 da Vibra", help="Consulta completa do item 8.4"):
            st.session_state['query_exemplo'] = "Descreva detalhadamente o item 8.4 do formul√°rio de refer√™ncia da Vibra"
    
    with col2:
        if st.button("‚ö° Vesting da CCR", help="Consulta espec√≠fica sobre vesting"):
            st.session_state['query_exemplo'] = "Como funciona o vesting nos planos de a√ß√µes restritas da CCR?"
    
    with col3:
        if st.button("üí∞ Liquida√ß√£o Vale", help="Consulta sobre forma de liquida√ß√£o"):
            st.session_state['query_exemplo'] = "Qual a forma de liquida√ß√£o dos planos da Vale?"
    
    st.divider()
    
    # --- √ÅREA DE CONSULTA ---
    st.subheader("üí¨ Fa√ßa sua Pergunta")
    
    user_query = st.text_area(
        "Digite sua pergunta:",
        value=st.session_state.get('query_exemplo', ''),
        height=100,
        placeholder="Ex: Descreva a forma de liquida√ß√£o das a√ß√µes restritas na Vibra"
    )
    
    # Limpa query_exemplo ap√≥s usar
    if 'query_exemplo' in st.session_state:
        del st.session_state['query_exemplo']
    
    if st.button("üîç Analisar", type="primary", disabled=not user_query.strip()):
        if not user_query.strip():
            st.warning("Por favor, digite uma pergunta.")
            return
        
        try:
            # ETAPA 1: Gera√ß√£o do plano
            st.header("üìã Plano de An√°lise Din√¢mico")
            with st.expander("Ver detalhes do planejamento", expanded=True):
                st.write("üîç Processando pergunta...")
                
                plan_response = create_dynamic_analysis_plan(
                    user_query, 
                    company_catalog, 
                    list(loaded_artifacts.keys())
                )
                
                if plan_response['status'] != 'success':
                    st.error("‚ùå Erro ao gerar plano de an√°lise")
                    return
                
                plan = plan_response['plan']
                
                # Exibe o plano gerado
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**üè¢ Empresas Identificadas:**")
                    for empresa in plan.get('empresas', []):
                        st.write(f"- {empresa}")
                
                with col2:
                    st.write("**üìù T√≥picos de An√°lise:**")
                    for i, topico in enumerate(plan.get('topicos', [])[:10]):
                        st.write(f"{i+1}. {topico}")
                    if len(plan.get('topicos', [])) > 10:
                        st.write(f"... e mais {len(plan.get('topicos', [])) - 10} t√≥picos")
                
                if not plan.get("empresas"):
                    st.error("‚ùå N√£o consegui identificar empresas na sua pergunta. Seja mais espec√≠fico.")
                    return
            
            # Detecta inten√ß√£o da query (MANTIDO ORIGINAL)
            query_intent = 'item_8_4_query' if ('8.4' in user_query.lower() or '8-4' in user_query.lower() or 
                                               'item 8.4' in user_query.lower() or 'formul√°rio' in user_query.lower()) else 'general_query'
            
            st.info(f"**Estrat√©gia detectada:** {query_intent}")
            
            # ETAPA 2: Execu√ß√£o do plano
            st.header("üîç Recupera√ß√£o de Contexto")
            with st.expander("Ver detalhes da busca", expanded=True):
                retrieved_context, sources = execute_dynamic_plan(
                    plan, query_intent, loaded_artifacts, embedding_model
                )
                
                if not retrieved_context.strip():
                    st.warning("‚ö†Ô∏è N√£o encontrei informa√ß√µes relevantes nos documentos.")
                    return
                
                st.success(f"‚úÖ Contexto recuperado de {len(set(sources))} documento(s)")
            
            # ETAPA 3: Gera√ß√£o da resposta final
            st.header("üìÑ Resposta Anal√≠tica")
            
            with st.spinner("Gerando resposta final..."):
                final_answer = get_final_unified_answer(user_query, retrieved_context)
            
            # Exibe a resposta
            st.markdown(final_answer)
            
            # Exibe fontes consultadas
            st.divider()
            with st.expander(f"üìö Documentos Consultados ({len(set(sources))})"):
                try:
                    unique_sources = sorted(list(set([str(source) for source in sources])))
                    for source in unique_sources:
                        st.write(f"- {source}")
                except Exception as e:
                    st.error(f"Erro ao processar fontes: {e}")
                    if sources:
                        st.write(f"Fontes: {list(set(sources))}")
        
        except Exception as e:
            st.error(f"‚ùå Erro durante a execu√ß√£o: {e}")
            st.exception(e)

if __name__ == "__main__":
    main()
