# -*- coding: utf-8 -*-
"""
AGENTE DE AN√ÅLISE LTIP - VERS√ÉO STREAMLIT (H√çBRIDO)
Aplica√ß√£o web para an√°lise de planos de incentivo de longo prazo, com
capacidades de busca profunda (RAG) e an√°lise agregada (resumo).
"""

import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import requests
import glob
import os
import re
import unicodedata
import logging
import pandas as pd
from pathlib import Path

try:
    from knowledge_base import DICIONARIO_UNIFICADO_HIERARQUICO
except ImportError:
    st.error("ERRO CR√çTICO: Crie o arquivo 'knowledge_base.py' e cole o 'DICIONARIO_UNIFICADO_HIERARQUICO' nele.")
    st.stop()


# --- FUN√á√ïES AUXILIARES GLOBAIS ---
# Estas fun√ß√µes s√£o usadas pelo fluxo de an√°lise profunda (RAG)



def normalize_name(name):
    """Normaliza nomes de empresas removendo acentos, pontua√ß√£o e sufixos comuns."""
    try:
        # Converte para min√∫sculas e remove acentos
        nfkd_form = unicodedata.normalize('NFKD', name.lower())
        name = "".join([c for c in nfkd_form if not unicodedata.combining(c)])
        
        # Remove pontua√ß√£o e caracteres especiais
        name = re.sub(r'[.,-]', '', name)
        
        # Remove sufixos comuns de empresas
        suffixes = [r'\bs\.?a\.?\b', r'\bltda\b', r'\bholding\b', r'\bparticipacoes\b', r'\bcia\b', r'\bind\b', r'\bcom\b']
        for suffix in suffixes:
            name = re.sub(suffix, '', name, flags=re.IGNORECASE)
            
        # Remove espa√ßos extras
        return re.sub(r'\s+', ' ', name).strip()
    except Exception as e:
        # Fallback em caso de erro
        return name.lower()

# --- CONFIGURA√á√ïES GERAIS ---
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
TOP_K_SEARCH = 7
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")
GEMINI_MODEL = "gemini-2.0-flash-lite"  # Modelo Gemini unificado
DADOS_PATH = "dados" # Centraliza o caminho para a pasta de dados

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)




# --- CARREGAMENTO DE DADOS E CACHING ---


@st.cache_resource
def load_all_artifacts():
    """
    (MODIFICADO) Garante que os artefatos de dados existam localmente,
    baixando-os do GitHub Releases se necess√°rio, antes de carregar na mem√≥ria.
    """
    DADOS_PATH.mkdir(exist_ok=True)

    ARQUIVOS_REMOTOS = {
        "item_8_4_chunks_map_final.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/item_8_4_chunks_map_final.json",
        "item_8_4_faiss_index_final.bin": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/item_8_4_faiss_index_final.bin",
        "outros_documentos_chunks_map_final.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/outros_documentos_chunks_map_final.json",
        "outros_documentos_faiss_index_final.bin": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/outros_documentos_faiss_index_final.bin",
        "resumo_fatos_e_topicos_final_enriquecido.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/resumo_fatos_e_topicos_final_enriquecido.json"
    }

    for nome_arquivo, url in ARQUIVOS_REMOTOS.items():
        caminho_arquivo = DADOS_PATH / nome_arquivo
        if not caminho_arquivo.exists():
            with st.spinner(f"Baixando artefato: {nome_arquivo}..."):
                try:
                    r = requests.get(url, stream=True)
                    r.raise_for_status()
                    with open(caminho_arquivo, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192):
                            f.write(chunk)
                    logger.info(f"Arquivo {nome_arquivo} baixado.")
                except requests.exceptions.RequestException as e:
                    st.error(f"Falha ao baixar {nome_arquivo}: {e}")
                    st.stop()

    st.success("Artefatos de dados prontos para uso.")

    model = SentenceTransformer(MODEL_NAME)
    artifacts = {}
    index_files = glob.glob(os.path.join(DADOS_PATH, '*_faiss_index_final.bin'))
    for index_file_path in index_files:
        category = os.path.basename(index_file_path).replace('_faiss_index_final.bin', '')
        chunks_file_path = DADOS_PATH / f"{category}_chunks_map_final.json"
        try:
            index = faiss.read_index(index_file_path)
            with open(chunks_file_path, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            artifacts[category] = {'index': index, 'chunks': chunk_data}
        except Exception as e:
            logger.error(f"Erro ao carregar '{category}': {e}")
            continue

    summary_data = None
    summary_file_path = DADOS_PATH / 'resumo_fatos_e_topicos_final_enriquecido.json'
    try:
        with open(summary_file_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
    except FileNotFoundError:
        logger.error(f"Arquivo de resumo '{summary_file_path}' n√£o encontrado.")

    return model, artifacts, summary_data

# --- FUN√á√ïES DE L√ìGICA DE NEG√ìCIO (ROTEADOR E MANIPULADORES) ---

def handle_aggregate_query(query: str, summary_data: dict, alias_map: dict):
    """
    (VERS√ÉO APRIMORADA) Processa uma pergunta agregada (ex: "Quais empresas t√™m RSU?").
    
    Esta fun√ß√£o foi reescrita para utilizar o formato do 'resumo enriquecido', que cont√©m
    os aliases espec√≠ficos encontrados para cada t√≥pico em cada empresa. Isso permite
    uma filtragem muito mais precisa e poderosa.

    Args:
        query (str): A pergunta feita pelo usu√°rio.
        summary_data (dict): O dicion√°rio carregado do JSON de resumo enriquecido.
        alias_map (dict): O mapa que converte qualquer alias para seu t√≥pico can√¥nico.
                          (gerado pela fun√ß√£o criar_mapa_de_alias).
    """
    
    # --- ETAPA 1: Identificar os termos-chave na pergunta do usu√°rio ---
    query_lower = query.lower()
    query_keywords = set()
    
    # Ordena os aliases conhecidos pelo comprimento, do maior para o menor.
    # Isso garante que "a√ß√µes restritas" seja encontrado antes de "a√ß√µes", evitando ambiguidades.
    sorted_aliases = sorted(alias_map.keys(), key=len, reverse=True)
    
    temp_query = query_lower
    for alias in sorted_aliases:
        # Usa regex com `\b` para garantir que estamos combinando palavras inteiras.
        if re.search(r'\b' + re.escape(alias) + r'\b', temp_query):
            query_keywords.add(alias)
            # Remove o alias encontrado da string de busca para n√£o cont√°-lo novamente.
            temp_query = temp_query.replace(alias, "")

    # Se nenhum termo conhecido for encontrado, informa o usu√°rio e encerra.
    if not query_keywords:
        st.warning("N√£o consegui identificar um termo t√©cnico conhecido (como 'TSR', 'vesting', 'RSU') na sua pergunta.")
        return

    st.info(f"Termos espec√≠ficos identificados para a busca: **{', '.join(sorted(list(query_keywords)))}**")

    # --- ETAPA 2: Filtrar as empresas usando a nova estrutura de dados enriquecida ---
    empresas_encontradas = []
    
    # Itera sobre cada empresa no nosso arquivo de resumo.
    for empresa, data in summary_data.items():
        
        # Para cada empresa, cria um conjunto com todos os seus termos conhecidos.
        company_terms = set()
        
        # Acessa a nova estrutura aninhada: {Se√ß√£o: {T√≥pico: [alias1, alias2]}}
        topicos_encontrados = data.get("topicos_encontrados", {})
        for section, topics in topicos_encontrados.items():
            for topic, aliases in topics.items():
                # Adiciona o nome do t√≥pico can√¥nico (ex: 'AcoesRestritas')
                company_terms.add(topic.lower())
                # Adiciona todos os aliases espec√≠ficos encontrados para aquele t√≥pico (ex: 'rsu')
                company_terms.update([a.lower() for a in aliases])
        
        # A condi√ß√£o de filtro: a empresa √© uma candidata se TODOS os termos da pergunta
        # do usu√°rio estiverem presentes no conjunto de termos da empresa.
        if query_keywords.issubset(company_terms):
            empresas_encontradas.append(empresa)

    # --- ETAPA 3: Apresentar os resultados ao usu√°rio ---
    if not empresas_encontradas:
        st.warning(f"Nenhuma empresa foi encontrada com **TODOS** os termos espec√≠ficos mencionados: `{', '.join(query_keywords)}`.")
        return

    st.success(f"‚úÖ **{len(empresas_encontradas)} empresa(s)** encontrada(s) com os crit√©rios definidos.")
    
    # Usa o Pandas para criar uma tabela bonita e organizada com os resultados.
    df = pd.DataFrame(sorted(empresas_encontradas), columns=["Empresa"])
    st.dataframe(df, use_container_width=True, hide_index=True)

def handle_rag_query(query, artifacts, model, company_catalog_rich):
    """
    Lida com perguntas detalhadas e comparativas usando o fluxo RAG completo.
    """
    # ETAPA 1: GERA√á√ÉO DO PLANO
    with st.status("1Ô∏è‚É£ Gerando plano de an√°lise...", expanded=True) as status:
        # Nota: Idealmente, company_catalog_rich seria carregado uma vez fora.
        # Por simplicidade, mantemos aqui.
        plan_response = create_dynamic_analysis_plan_v2(query, company_catalog_rich, list(artifacts.keys()))
        if plan_response['status'] != "success" or not plan_response['plan']['empresas']:
            st.error("‚ùå N√£o consegui identificar empresas na sua pergunta. Tente usar nomes conhecidos (ex: Magalu, Vivo, Ita√∫).")
            return "An√°lise abortada.", set()
        
        plan = plan_response['plan']
        empresas = plan.get('empresas', [])
        st.write(f"**üè¢ Empresas identificadas:** {', '.join(empresas)}")
        st.write(f"**üìù T√≥picos a analisar:** {len(plan.get('topicos', []))}")
        status.update(label="‚úÖ Plano gerado com sucesso!", state="complete")

    # ETAPA 2: L√ìGICA DE EXECU√á√ÉO (com tratamento para compara√ß√µes)
    final_answer = ""
    sources = set()

    # MODO COMPARATIVO
    if len(empresas) > 1:
        st.info(f"Modo de compara√ß√£o ativado para {len(empresas)} empresas. Analisando sequencialmente...")
        summaries = []
        for i, empresa in enumerate(empresas):
            with st.status(f"Analisando {i+1}/{len(empresas)}: {empresa}...", expanded=True):
                single_company_plan = {'empresas': [empresa], 'topicos': plan['topicos']}
                query_intent = 'item_8_4_query' if any(term in query.lower() for term in ['8.4', 'formul√°rio']) else 'general_query'
                retrieved_context, retrieved_sources = execute_dynamic_plan(single_company_plan, query_intent, artifacts, model)
                sources.update(retrieved_sources)

                if "Nenhuma informa√ß√£o" in retrieved_context or not retrieved_context.strip():
                    summary = f"## An√°lise para {empresa.upper()}\n\nNenhuma informa√ß√£o encontrada nos documentos para os t√≥picos solicitados."
                else:
                    summary_prompt = f"Com base no contexto a seguir sobre a empresa {empresa}, resuma os pontos principais sobre os seguintes t√≥picos: {', '.join(plan['topicos'])}. Contexto: {retrieved_context}"
                    summary = get_final_unified_answer(summary_prompt, retrieved_context)
                
                summaries.append(f"--- RESUMO PARA {empresa.upper()} ---\n\n{summary}")

        with st.status("Gerando relat√≥rio comparativo final...", expanded=True):
            comparison_prompt = f"Com base nos resumos individuais a seguir, crie um relat√≥rio comparativo detalhado e bem estruturado entre as empresas, focando nos pontos levantados na pergunta original do usu√°rio.\n\nPergunta original do usu√°rio: '{query}'\n\n" + "\n\n".join(summaries)
            final_answer = get_final_unified_answer(comparison_prompt, "\n\n".join(summaries))
            status.update(label="‚úÖ Relat√≥rio comparativo gerado!", state="complete")

    # MODO DE AN√ÅLISE √öNICA
    else:
        with st.status("2Ô∏è‚É£ Recuperando contexto relevante...", expanded=True) as status:
            query_intent = 'item_8_4_query' if any(term in query.lower() for term in ['8.4', 'formul√°rio']) else 'general_query'
            st.write(f"**üéØ Estrat√©gia detectada:** {'Item 8.4 completo' if query_intent == 'item_8_4_query' else 'Busca geral'}")
            
            retrieved_context, retrieved_sources = execute_dynamic_plan(plan, query_intent, artifacts, model)
            sources.update(retrieved_sources)
            
            if not retrieved_context.strip() or "Nenhuma informa√ß√£o encontrada" in retrieved_context:
                st.error("‚ùå N√£o encontrei informa√ß√µes relevantes nos documentos para a sua consulta.")
                return "Nenhuma informa√ß√£o relevante encontrada.", set()
            
            st.write(f"**üìÑ Contexto recuperado de:** {len(sources)} documento(s)")
            status.update(label="‚úÖ Contexto recuperado com sucesso!", state="complete")
        
        with st.status("3Ô∏è‚É£ Gerando resposta final...", expanded=True) as status:
            final_answer = get_final_unified_answer(query, retrieved_context)
            status.update(label="‚úÖ An√°lise conclu√≠da!", state="complete")

    return final_answer, sources

def handle_direct_fact_query(query: str, summary_data: dict, alias_map: dict, company_catalog: list):
    """(NOVO) Responde a perguntas diretas de fatos usando o resumo."""
    query_lower = query.lower()
    empresa_encontrada, fato_encontrado_alias = None, None

    # Tenta extrair a empresa e o fato da pergunta
    for company_data in company_catalog:
        for alias in company_data.get("aliases", []):
            if re.search(r'\b' + re.escape(alias.lower()) + r'\b', query_lower):
                empresa_encontrada = company_data["canonical_name"].upper()
                break
        if empresa_encontrada: break

    for alias in sorted(alias_map.keys(), key=len, reverse=True):
        if re.search(r'\b' + re.escape(alias) + r'\b', query_lower):
            fato_encontrado_alias = alias
            break

    # Se n√£o encontrou empresa e fato, n√£o √© uma query de fato direto
    if not empresa_encontrada or not fato_encontrado_alias:
        return False

    # Busca o fato nos dados da empresa
    empresa_data = summary_data.get(empresa_encontrada, {})
    st.subheader(f"Fato Direto para: {empresa_encontrada}")
    fato_encontrado = False
    for fact_key, fact_value in empresa_data.get("fatos_extraidos", {}).items():
        if fato_encontrado_alias in fact_key.lower():
            valor = fact_value.get('valor', '')
            unidade = fact_value.get('unidade', '')
            st.metric(label=f"Fato: {fact_key.replace('_', ' ').title()}", value=f"{valor} {unidade}".strip())
            fato_encontrado = True
            break

    if not fato_encontrado:
        st.info(f"O t√≥pico '{fato_encontrado_alias}' foi mencionado, mas um fato estruturado (com valor) n√£o p√¥de ser extra√≠do.")

    return True # Query tratada com sucesso

# --- FUN√á√ïES DE BACKEND (RAG) - com modelo atualizado ---
@st.cache_data
def criar_mapa_de_alias():
    """(MODIFICADO) Cria o mapa de alias a partir do dicion√°rio hier√°rquico unificado."""
    alias_to_canonical = {}
    if 'DICIONARIO_UNIFICADO_HIERARQUICO' not in globals():
         return {} # Evita erro se o dicion√°rio n√£o for importado
    for section, topics in DICIONARIO_UNIFICADO_HIERARQUICO.items():
        for canonical_name, aliases in topics.items():
            alias_to_canonical[canonical_name.lower()] = canonical_name
            for alias in aliases:
                alias_to_canonical[alias.lower()] = canonical_name
    return alias_to_canonical

def create_dynamic_analysis_plan_v2(query, company_catalog_rich, available_indices):
    # Esta fun√ß√£o agora √© chamada apenas pelo `handle_rag_query`
    api_key = GEMINI_API_KEY
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={api_key}"
    query_lower = query.lower().strip()
    
    # Identifica√ß√£o de Empresas
    mentioned_companies = []
    companies_found_by_alias = {}
    if company_catalog_rich:
        for company_data in company_catalog_rich:
            for alias in company_data.get("aliases", []):
                if re.search(r'\b' + re.escape(alias.lower()) + r'\b', query_lower):
                    score = len(alias.split())
                    canonical_name = company_data["canonical_name"]
                    if canonical_name not in companies_found_by_alias or score > companies_found_by_alias[canonical_name]:
                        companies_found_by_alias[canonical_name] = score
        if companies_found_by_alias:
            sorted_companies = sorted(companies_found_by_alias.items(), key=lambda item: item[1], reverse=True)
            mentioned_companies = [company for company, score in sorted_companies]
    
    if not mentioned_companies:
        return {"status": "error", "plan": {}}
    
    # Identifica√ß√£o de T√≥picos
    topics = []
    found_topics = set()
    alias_map = criar_mapa_de_alias() # Reutiliza o mapa de alias
    for alias, canonical_name in alias_map.items():
        if re.search(r'\b' + re.escape(alias) + r'\b', query_lower):
            found_topics.add(canonical_name)

    if found_topics:
        topics = list(found_topics)
    else:
        # Fallback para LLM se nenhum t√≥pico for encontrado
        prompt = f"""Voc√™ √© um consultor de ILP. Identifique os T√ìPICOS CENTRAIS da pergunta: "{query}".
        Retorne APENAS uma lista JSON com os t√≥picos mais relevantes de: {json.dumps(AVAILABLE_TOPICS)}.
        Se for gen√©rica, selecione t√≥picos para uma an√°lise geral. Formato: ["T√≥pico 1", "T√≥pico 2"]"""
        payload = {"contents": [{"parts": [{"text": prompt}]}]}
        headers = {'Content-Type': 'application/json'}
        try:
            response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=90)
            response.raise_for_status()
            text_response = response.json()['candidates'][0]['content']['parts'][0]['text']
            json_match = re.search(r'\[.*\]', text_response, re.DOTALL)
            if json_match:
                topics = json.loads(json_match.group(0))
            else:
                topics = ["Estrutura do Plano/Programa", "Vesting", "Op√ß√µes de Compra de A√ß√µes"]
        except Exception as e:
            logger.error(f"Falha ao chamar LLM para t√≥picos: {e}")
            topics = ["Estrutura do Plano/Programa", "Vesting", "Op√ß√µes de Compra de A√ß√µes"]
            
    plan = {"empresas": mentioned_companies, "topicos": topics}
    return {"status": "success", "plan": plan}


def execute_dynamic_plan(plan, query_intent, artifacts, model):
    """
    Executa o plano de busca com controle robusto de tokens e deduplica√ß√£o.
    """
    full_context = ""
    all_retrieved_docs = set()
    unique_chunks_content = set()
    current_token_count = 0
    chunks_processed = 0

    class Config: # Usando uma classe interna para manter as constantes da fun√ß√£o
        MAX_CONTEXT_TOKENS = 256000
        MAX_CHUNKS_PER_TOPIC = 10
        SCORE_THRESHOLD_GENERAL = 0.4
        SCORE_THRESHOLD_ITEM_84 = 0.5
        DEDUPLICATION_HASH_LENGTH = 100

    def estimate_tokens(text):
        return len(text) // 4

    def generate_chunk_hash(chunk_text):
        normalized = re.sub(r'\s+', '', chunk_text.lower())
        return hash(normalized[:Config.DEDUPLICATION_HASH_LENGTH])

    def add_unique_chunk_to_context(chunk_text, source_info):
        nonlocal full_context, current_token_count, chunks_processed, unique_chunks_content, all_retrieved_docs
        
        chunk_hash = generate_chunk_hash(chunk_text)
        if chunk_hash in unique_chunks_content:
            logger.debug(f"Chunk duplicado ignorado: {source_info[:50]}...")
            return "DUPLICATE"
        
        estimated_chunk_tokens = estimate_tokens(chunk_text) + estimate_tokens(source_info) + 10
        
        if current_token_count + estimated_chunk_tokens > Config.MAX_CONTEXT_TOKENS:
            logger.warning(f"Limite de tokens atingido. Atual: {current_token_count}")
            return "LIMIT_REACHED"
        
        unique_chunks_content.add(chunk_hash)
        full_context += f"--- {source_info} ---\n{chunk_text}\n\n"
        current_token_count += estimated_chunk_tokens
        chunks_processed += 1
        
        try:
            doc_name = source_info.split("(Doc: ")[1].split(")")[0]
            all_retrieved_docs.add(doc_name)
        except IndexError:
            pass # Ignora se n√£o conseguir extrair o nome
        
        logger.debug(f"Chunk adicionado. Tokens atuais: {current_token_count}")
        return "SUCCESS"

    for empresa in plan.get("empresas", []):
        searchable_company_name = unicodedata.normalize('NFKD', empresa.lower()).encode('ascii', 'ignore').decode('utf-8').split(' ')[0]
        logger.info(f"Processando empresa: {empresa}")

        # L√≥gica para busca geral (pode ser adaptada para item_8_4 tamb√©m)
        full_context += f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
        
        # Busca por tags
        target_tags = []
        for topico in plan.get("topicos", []):
            target_tags.extend(expand_search_terms(topico))
        target_tags = list(set([tag.title() for tag in target_tags if len(tag) > 3]))
        
        # A fun√ß√£o search_by_tags precisa estar definida no seu script
        tagged_chunks = search_by_tags(artifacts, empresa, target_tags)
        
        if tagged_chunks:
            full_context += f"=== CHUNKS COM TAGS ESPEC√çFICAS - {empresa.upper()} ===\n\n"
            for chunk_info in tagged_chunks:
                add_unique_chunk_to_context(
                    chunk_info['text'], 
                    f"Chunk com tag '{chunk_info['tag_found']}' (Doc: {chunk_info['path']})"
                )

        # Busca sem√¢ntica complementar
        for topico in plan.get("topicos", []):
            expanded_terms = expand_search_terms(topico)
            for term in expanded_terms[:3]:
                search_query = f"informa√ß√µes sobre {term} no plano de remunera√ß√£o da empresa {empresa}"
                query_embedding = model.encode([search_query], normalize_embeddings=True).astype('float32')
                
                for index_name, artifact_data in artifacts.items():
                    index = artifact_data['index']
                    chunk_data = artifact_data['chunks']
                    scores, indices = index.search(query_embedding, TOP_K_SEARCH)
                    
                    for i, idx in enumerate(indices[0]):
                        if idx != -1 and scores[0][i] > Config.SCORE_THRESHOLD_GENERAL:
                            document_path = chunk_data["map"][idx]['document_path']
                            if searchable_company_name in document_path.lower():
                                chunk_text = chunk_data["chunks"][idx]
                                add_unique_chunk_to_context(
                                    chunk_text,
                                    f"Contexto para '{topico}' via '{term}' (Fonte: {index_name}, Score: {scores[0][i]:.3f}, Doc: {document_path})"
                                )

    if not unique_chunks_content:
        logger.warning("Nenhum chunk √∫nico encontrado para o plano de execu√ß√£o.")
        return "Nenhuma informa√ß√£o √∫nica encontrada para os crit√©rios especificados.", set()

    logger.info(f"Processamento conclu√≠do - Tokens: {current_token_count}, Chunks √∫nicos: {len(unique_chunks_content)}")
    return full_context, all_retrieved_docs

def get_final_unified_answer(query, context):
    """Gera a resposta final usando o contexto recuperado e a API do Gemini."""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    
    has_complete_8_4 = "=== SE√á√ÉO COMPLETA DO ITEM 8.4" in context
    has_tagged_chunks = "=== CHUNKS COM TAGS ESPEC√çFICAS" in context
    
    structure_instruction = "Organize a resposta de forma l√≥gica e clara usando Markdown."
    if has_complete_8_4:
        structure_instruction = """
**ESTRUTURA OBRIGAT√ìRIA PARA ITEM 8.4:**
Use a estrutura oficial do item 8.4 do Formul√°rio de Refer√™ncia:
a) Termos e condi√ß√µes gerais; b) Data de aprova√ß√£o e √≥rg√£o; c) M√°ximo de a√ß√µes; d) M√°ximo de op√ß√µes; 
e) Condi√ß√µes de aquisi√ß√£o; f) Crit√©rios de pre√ßo; g) Crit√©rios de prazo; h) Forma de liquida√ß√£o; 
i) Restri√ß√µes √† transfer√™ncia; j) Suspens√£o/extin√ß√£o; k) Efeitos da sa√≠da.
Para cada subitem, extraia e organize as informa√ß√µes encontradas na SE√á√ÉO COMPLETA DO ITEM 8.4.
"""
    elif has_tagged_chunks:
        structure_instruction = "**PRIORIZE** as informa√ß√µes dos CHUNKS COM TAGS ESPEC√çFICAS e organize a resposta de forma l√≥gica usando Markdown."
        
    prompt = f"""Voc√™ √© um consultor especialista em planos de incentivo de longo prazo (ILP) e no item 8 do formul√°rio de refer√™ncia da CVM.
    
    PERGUNTA ORIGINAL DO USU√ÅRIO: "{query}"
    
    CONTEXTO COLETADO DOS DOCUMENTOS:
    {context}
    
    {structure_instruction}
    
    INSTRU√á√ïES PARA O RELAT√ìRIO FINAL:
    1. Responda diretamente √† pergunta do usu√°rio com base no contexto fornecido.
    2. PRIORIZE informa√ß√µes da "SE√á√ÉO COMPLETA DO ITEM 8.4" ou de "CHUNKS COM TAGS ESPEC√çFICAS" quando dispon√≠veis. Use o resto do contexto para complementar.
    3. Seja detalhado, preciso e profissional na sua linguagem. Use formata√ß√£o Markdown (negrito, listas) para clareza.
    4. Se uma informa√ß√£o espec√≠fica pedida n√£o estiver no contexto, declare explicitamente: "Informa√ß√£o n√£o encontrada nas fontes analisadas.". N√£o invente dados.
    
    RELAT√ìRIO ANAL√çTICO FINAL:
    """
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}
    }
    headers = {'Content-Type': 'application/json'}
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
    except Exception as e:
        logger.error(f"ERRO ao gerar resposta final com LLM: {e}")
        return f"ERRO ao gerar resposta final: Ocorreu um problema ao contatar o modelo de linguagem. Detalhes: {e}"


# --- INTERFACE STREAMLIT (Aplica√ß√£o Principal) ---
def main():
    st.set_page_config(page_title="Agente de An√°lise LTIP", page_icon="üîç", layout="wide", initial_sidebar_state="expanded")
    st.title("ü§ñ Agente de An√°lise de Planos de Incentivo (ILP)")
    st.markdown("---")

    # Carregamento centralizado dos artefatos
    model, artifacts, summary_data = load_all_artifacts()
    ALIAS_MAP = criar_mapa_de_alias()

    company_catalog = [{"canonical_name": name, "aliases": [name.split(' ')[0], name]} for name in summary_data.keys()]



    except ImportError:
        company_catalog_rich = []
        logger.warning("`catalog_data.py` n√£o encontrado. A identifica√ß√£o de empresas por apelidos ser√° limitada.")

    # Valida√ß√£o dos dados carregados
    if not artifacts:
        st.error("‚ùå Erro cr√≠tico: Nenhum artefato de busca (√≠ndices FAISS) foi carregado. A an√°lise profunda est√° desativada.")
    if not summary_data:
        st.warning("‚ö†Ô∏è Aviso: O arquivo `resumo_caracteristicas.json` n√£o foi encontrado. An√°lises de 'quais/quantas empresas' est√£o desativadas.")
    
    # --- Sidebar ---
    with st.sidebar:
        st.header("üìä Informa√ß√µes do Sistema")
        st.metric("Fontes de Documentos (RAG)", len(artifacts) if artifacts else 0)
        st.metric("Empresas no Resumo", len(summary_data) if summary_data else 0)
        
        if summary_data:
            with st.expander("empresas com caracter√≠sticas identificadas"):
                st.dataframe(sorted(list(summary_data.keys())), use_container_width=True)
        
        st.success("‚úÖ Sistema pronto para an√°lise")
        st.info(f"Embedding Model: `{MODEL_NAME}`")
        st.info(f"Generative Model: `{GEMINI_MODEL}`") # Mostra o modelo Gemini em uso

    # --- Corpo Principal ---
    st.header("üí¨ Fa√ßa sua pergunta")
    
        # Colunas para exemplos de perguntas
    col1, col2 = st.columns(2)
    with col1:
        st.info("**Experimente uma an√°lise agregada:**")
        st.code("Quais empresas possuem planos com matching?")
        st.code("Quantas empresas t√™m performance?")
        st.code("Quantas empresas t√™m Stock Options?")
    with col2:
        st.info("**Ou uma an√°lise profunda :**")
        st.code("Compare dividendos da Vale com a Gerdau")
        st.code("Como √© o plano Magazine Luiza?")
        st.code("Resumo item 8.4 Movida")
    st.caption("**Principais Termos-Chave:** `Item 8.4`, `Vesting`, `Stock Options`, `A√ß√µes Restritas`, `Performance`, `Matching`, `Lockup`, `SAR`, `ESPP`, `Malus e Clawback`, `Dividendos`")

    user_query = st.text_area("Sua pergunta:", height=100, placeholder="Ex: Quantas empresas oferecem a√ß√µes restritas? ")

    if st.button("üîç Analisar", type="primary", use_container_width=True):
        if not user_query.strip():
            st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            return

        st.markdown("---")
        st.subheader("üìã Resultado da An√°lise")
        
        # --- O ROTEADOR DE INTEN√á√ÉO EM A√á√ÉO ---
        final_answer = ""
        sources = set()
        
        query_lower = user_query.lower()
        aggregate_keywords = ["quais", "quantas", "liste", "qual a lista de"]

        # Rota 1: Pergunta agregada
        query_lower = user_query.lower()
        aggregate_keywords = ["quais", "quantas", "liste"]
        # Padr√£o simples para detectar perguntas como "Qual o/a ... da/de ..."
        direct_fact_pattern = r'qual\s*(?:√©|o|a)\s*.*\s*d[aeo]\s*'

        # N√≠vel 1: Pergunta Agregada
        if any(keyword in query_lower for keyword in aggregate_keywords):
            handle_aggregate_query(user_query, summary_data, ALIAS_MAP)

        # N√≠vel 2: Pergunta de Fato Direto
        elif re.search(direct_fact_pattern, query_lower):
            st.info("Buscando resposta direta no resumo...")
            # A fun√ß√£o retorna True se tratou a query, False se n√£o
            if not handle_direct_fact_query(user_query, summary_data, ALIAS_MAP, company_catalog):
                 # Se n√£o conseguiu tratar, cai para o RAG
                 st.info("N√£o foi poss√≠vel responder diretamente. Acionando an√°lise profunda...")
                 final_answer, sources = handle_rag_query(user_query, artifacts, model, company_catalog) # Passe o company_catalog aqui tamb√©m
                 st.markdown(final_answer)

        # N√≠vel 3: Pergunta Complexa (RAG)
        else:
            final_answer, sources = handle_rag_query(user_query, artifacts, model, company_catalog) # Passe o company_catalog aqui tamb√©m
            st.markdown(final_answer)
            if 'sources' in locals() and sources:
                 with st.expander(f"üìö Documentos consultados ({len(sources)})"):
                      st.write(sorted(list(sources)))
        # Fontes consultadas (apenas para o RAG)
        if sources:
            st.markdown("---")
            with st.expander(f"üìö Documentos consultados na an√°lise profunda ({len(sources)})", expanded=False):
                for i, source in enumerate(sorted(list(sources)), 1):
                    st.write(f"{i}. {source}")

if __name__ == "__main__":
    main()
