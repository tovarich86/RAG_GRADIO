# app.py
import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import requests
import glob
import os
import re
import unicodedata

# --- CONFIGURA√á√ÉO DA P√ÅGINA ---
st.set_page_config(
    page_title="üöÄ Agente de An√°lise LTIP",
    page_icon="üöÄ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- CONFIGURA√á√ïES ---
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'

# CORRE√á√ÉO: M√∫ltiplos caminhos para encontrar a pasta data
POSSIBLE_DATA_PATHS = [
    './data',                                    # Caminho relativo padr√£o
    os.path.join(os.getcwd(), 'data'),          # Diret√≥rio atual + data
    os.path.join(os.path.dirname(__file__), 'data'),  # Pasta do script + data
    '/mount/src/agente-streamlit-web/data',     # Caminho absoluto Streamlit Cloud
    'data'                                      # Apenas 'data'
]

def find_data_directory():
    """Encontra a pasta data em diferentes localiza√ß√µes poss√≠veis"""
    for path in POSSIBLE_DATA_PATHS:
        if os.path.exists(path):
            # Verifica se tem arquivos FAISS
            faiss_files = glob.glob(os.path.join(path, '*_faiss_index.bin'))
            if faiss_files:
                st.info(f"‚úÖ Pasta data encontrada em: {path}")
                st.info(f"‚úÖ Arquivos FAISS encontrados: {len(faiss_files)}")
                return path
    
    st.error("‚ùå Pasta 'data' com arquivos FAISS n√£o encontrada!")
    
    # Debug: Mostra estrutura do diret√≥rio
    current_dir = os.getcwd()
    st.error(f"üìÇ Diret√≥rio atual: {current_dir}")
    
    try:
        files_in_current = os.listdir(current_dir)
        st.error(f"üìÅ Arquivos/pastas no diret√≥rio atual: {files_in_current}")
        
        # Se existe pasta data mas sem arquivos
        if 'data' in files_in_current:
            data_contents = os.listdir(os.path.join(current_dir, 'data'))
            st.error(f"üìÅ Conte√∫do da pasta data: {data_contents}")
    except Exception as e:
        st.error(f"Erro ao listar diret√≥rio: {e}")
    
    return None

# Dicion√°rios de termos t√©cnicos (mantidos do c√≥digo original)
TERMOS_TECNICOS_LTIP = {
    "tratamento de dividendos": ["tratamento de dividendos", "equivalente em dividendos", "dividendos", "juros sobre capital pr√≥prio", "proventos", "dividend equivalent", "dividendos pagos em a√ß√µes", "ajustes por dividendos"],
    "pre√ßo de exerc√≠cio": ["pre√ßo de exerc√≠cio", "strike price", "pre√ßo de compra", "pre√ßo fixo", "valor de exerc√≠cio", "pre√ßo pr√©-estabelecido", "pre√ßo de aquisi√ß√£o"],
    "forma de liquida√ß√£o": ["forma de liquida√ß√£o", "liquida√ß√£o", "pagamento", "entrega f√≠sica", "pagamento em dinheiro", "transfer√™ncia de a√ß√µes", "settlement"],
    "vesting": ["vesting", "per√≠odo de car√™ncia", "car√™ncia", "aquisi√ß√£o de direitos", "cronograma de vesting", "vesting schedule", "per√≠odo de cliff"],
    "eventos corporativos": ["eventos corporativos", "desdobramento", "grupamento", "dividendos pagos em a√ß√µes", "bonifica√ß√£o", "split", "ajustes", "reorganiza√ß√£o societ√°ria"],
    "stock options": ["stock options", "op√ß√µes de a√ß√µes", "op√ß√µes de compra", "SOP", "plano de op√ß√µes", "ESOP", "op√ß√£o de compra de a√ß√µes"],
    "a√ß√µes restritas": ["a√ß√µes restritas", "restricted shares", "RSU", "restricted stock units", "a√ß√µes com restri√ß√£o", "plano de a√ß√µes restritas"]
}

AVAILABLE_TOPICS = [
    "termos e condi√ß√µes gerais", "data de aprova√ß√£o e √≥rg√£o respons√°vel", "n√∫mero m√°ximo de a√ß√µes abrangidas", "n√∫mero m√°ximo de op√ß√µes a serem outorgadas", "condi√ß√µes de aquisi√ß√£o de a√ß√µes", "crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio", "pre√ßo de exerc√≠cio", "strike price", "crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio", "forma de liquida√ß√£o", "liquida√ß√£o", "pagamento", "restri√ß√µes √† transfer√™ncia das a√ß√µes", "crit√©rios e eventos de suspens√£o/extin√ß√£o", "efeitos da sa√≠da do administrador", "Tipos de Planos", "Condi√ß√µes de Car√™ncia", "Vesting", "per√≠odo de car√™ncia", "cronograma de vesting", "Matching", "contrapartida", "co-investimento", "Lockup", "per√≠odo de lockup", "restri√ß√£o de venda", "Tratamento de Dividendos", "equivalente em dividendos", "proventos", "Stock Options", "op√ß√µes de a√ß√µes", "SOP", "A√ß√µes Restritas", "RSU", "restricted shares", "Eventos Corporativos", "IPO", "grupamento", "desdobramento"
]

# --- FUN√á√ïES (mantidas do c√≥digo original) ---
def safe_api_call(url, payload, headers, timeout=90):
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=timeout)
        response.raise_for_status()
        return response.json(), None
    except requests.exceptions.HTTPError as e:
        return None, f"Erro de API com c√≥digo {e.response.status_code}: {e.response.reason}"
    except requests.exceptions.RequestException:
        return None, "Erro de conex√£o. Verifique sua internet."

def expand_search_terms(base_term):
    expanded_terms = [base_term.lower()]
    for category, terms in TERMOS_TECNICOS_LTIP.items():
        if any(term.lower() in base_term.lower() for term in terms):
            expanded_terms.extend([term.lower() for term in terms])
    return list(set(expanded_terms))

def search_by_tags(artifacts, company_name, target_tags):
    results = []
    for index_name, artifact_data in artifacts.items():
        chunk_data = artifact_data['chunks']
        for i, mapping in enumerate(chunk_data.get('map', [])):
            document_path = mapping['document_path']
            if re.search(re.escape(company_name.split(' ')[0]), document_path, re.IGNORECASE):
                chunk_text = chunk_data["chunks"][i]
                for tag in target_tags:
                    if f"T√≥picos:" in chunk_text and tag in chunk_text:
                        results.append({
                            'text': chunk_text, 'path': document_path, 'index': i,
                            'source': index_name, 'tag_found': tag
                        })
                        st.write(f"     -> Encontrado chunk com tag '{tag}' em {document_path}")
                        break
    return results

@st.cache_resource
def load_all_artifacts():
    """Carrega artefatos com detec√ß√£o autom√°tica de caminho"""
    artifacts = {}
    canonical_company_names = set()
    
    # CORRE√á√ÉO: Busca a pasta data automaticamente
    data_path = find_data_directory()
    if not data_path:
        return None, None, None
    
    st.info("üì¶ Carregando artefatos...")
    
    with st.spinner("Carregando modelo de embedding..."):
        model = SentenceTransformer(MODEL_NAME)
    
    # Busca arquivos FAISS
    index_files = glob.glob(os.path.join(data_path, '*_faiss_index.bin'))
    
    if not index_files:
        st.error(f"‚ùå Nenhum arquivo *_faiss_index.bin encontrado em: {data_path}")
        return None, None, None
    
    st.success(f"‚úÖ Encontrados {len(index_files)} arquivo(s) FAISS:")
    for file in index_files:
        st.write(f"  - {os.path.basename(file)}")
    
    progress_bar = st.progress(0)
    total_files = len(index_files)
    
    for idx, index_file in enumerate(index_files):
        category = os.path.basename(index_file).replace('_faiss_index.bin', '')
        chunks_file = os.path.join(data_path, f"{category}_chunks_map.json")
        
        try:
            st.info(f"Carregando '{category}'...")
            
            # Carrega o √≠ndice FAISS
            index = faiss.read_index(index_file)
            
            # Carrega os chunks correspondentes
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            
            artifacts[category] = {'index': index, 'chunks': chunk_data}
            
            # Extrai nomes das empresas
            for mapping in chunk_data.get('map', []):
                company_name = mapping['document_path'].split('/')[0]
                canonical_company_names.add(company_name)
            
            progress_bar.progress((idx + 1) / total_files)
            
        except FileNotFoundError:
            st.error(f"‚ùå Arquivo '{chunks_file}' n√£o encontrado!")
            continue
        except Exception as e:
            st.error(f"‚ùå Erro ao carregar '{category}': {e}")
            continue
    
    if not artifacts:
        st.error("‚ùå NENHUM artefato foi carregado com sucesso!")
        return None, None, None
    
    st.success(f"‚úÖ {len(artifacts)} categorias carregadas: {list(artifacts.keys())}")
    st.success(f"‚úÖ {len(canonical_company_names)} empresas identificadas")
    
    return artifacts, model, list(canonical_company_names)

# Resto das fun√ß√µes (create_dynamic_analysis_plan, execute_dynamic_plan, get_final_unified_answer)
# mantidas exatamente como no c√≥digo original...

def create_dynamic_analysis_plan(query, company_catalog, available_indices):
    """Cria plano de an√°lise usando API do Gemini"""
    
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
    except:
        api_key = st.session_state.get('gemini_api_key')
        if not api_key:
            st.error("‚ùå API Key do Gemini n√£o configurada!")
            return {"status": "error", "message": "API Key n√£o encontrada"}
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
    
    # Identifica√ß√£o de empresas (l√≥gica robusta do c√≥digo original)
    mentioned_companies = []
    query_clean = query.lower().strip()
    
    st.write(f"üîç Buscando empresas na query: '{query_clean}'")
    
    for canonical_name in company_catalog:
        if canonical_name.lower() in query_clean:
            mentioned_companies.append(canonical_name)
            st.write(f"   ‚úÖ Encontrada: {canonical_name}")
            continue
        
        company_parts = canonical_name.split(' ')
        for part in company_parts:
            if len(part) > 2 and part.lower() in query_clean:
                if canonical_name not in mentioned_companies:
                    mentioned_companies.append(canonical_name)
                    st.write(f"   ‚úÖ Encontrada por parte '{part}': {canonical_name}")
                break
    
    # Chamada para an√°lise de t√≥picos
    prompt = f"""
Voc√™ √© um planejador de an√°lise. Analise a pergunta e identifique os t√≥picos de interesse.

**Instru√ß√µes:**
- Se for pergunta gen√©rica (ex: "resumo dos planos"), inclua TODOS os t√≥picos
- Se for espec√≠fica (ex: "vesting da empresa X"), inclua apenas t√≥picos relevantes
- Retorne APENAS uma lista JSON de strings

**T√≥picos Dispon√≠veis:** {json.dumps(AVAILABLE_TOPICS, indent=2)}

**Pergunta:** "{query}"

**T√≥picos (JSON apenas):**
"""
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    
    response_data, error_message = safe_api_call(url, payload, headers)
    
    if error_message:
        st.warning(f"‚ö†Ô∏è {error_message}")
        plan = {"empresas": mentioned_companies, "topicos": AVAILABLE_TOPICS}
        return {"status": "success", "plan": plan}
    
    try:
        text_response = response_data['candidates'][0]['content']['parts'][0]['text']
        json_match = re.search(r'\[.*\]', text_response, re.DOTALL)
        if json_match:
            topics = json.loads(json_match.group(0))
            plan = {"empresas": mentioned_companies, "topicos": topics}
            return {"status": "success", "plan": plan}
        else:
            plan = {"empresas": mentioned_companies, "topicos": AVAILABLE_TOPICS}
            return {"status": "success", "plan": plan}
    except Exception as e:
        st.error(f"‚ùå Erro ao processar resposta: {e}")
        plan = {"empresas": mentioned_companies, "topicos": AVAILABLE_TOPICS}
        return {"status": "success", "plan": plan}

def execute_dynamic_plan(plan, query_intent, artifacts, model):
    """Executa o plano de busca (l√≥gica do c√≥digo original)"""
    full_context = ""
    all_retrieved_docs = set()
    
    if query_intent == 'item_8_4_query':
        st.write("üìã Estrat√©gia: BUSCA EXAUSTIVA ITEM 8.4")
        
        for empresa in plan.get("empresas", []):
            full_context += f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
            
            if 'item_8_4' in artifacts:
                artifact_data = artifacts['item_8_4']
                chunk_data = artifact_data['chunks']
                
                chunks_8_4 = []
                for i, mapping in enumerate(chunk_data.get('map', [])):
                    document_path = mapping['document_path']
                    if re.search(re.escape(empresa.split(' ')[0]), document_path, re.IGNORECASE):
                        chunk_text = chunk_data["chunks"][i]
                        all_retrieved_docs.add(str(document_path))
                        chunks_8_4.append({'text': chunk_text, 'path': document_path})
                
                st.write(f"   üìÑ {len(chunks_8_4)} chunks do Item 8.4 para {empresa}")
                
                full_context += f"=== SE√á√ÉO COMPLETA DO ITEM 8.4 - {empresa.upper()} ===\n\n"
                for chunk in chunks_8_4:
                    full_context += f"--- Chunk Item 8.4 (Doc: {chunk['path']}) ---\n{chunk['text']}\n\n"
                full_context += f"=== FIM DA SE√á√ÉO ITEM 8.4 - {empresa.upper()} ===\n\n"
    
    else:
        st.write("üìã Estrat√©gia: BUSCA GERAL COM TAGS")
        
        for empresa in plan.get("empresas", []):
            full_context += f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
            
            target_tags = []
            for topico in plan.get("topicos", []):
                target_tags.extend(expand_search_terms(topico))
            
            target_tags = list(set([tag.title() for tag in target_tags if len(tag) > 3]))
            st.write(f"   üè∑Ô∏è {len(target_tags)} tags para {empresa}")
            
            tagged_chunks = search_by_tags(artifacts, empresa, target_tags)
            
            if tagged_chunks:
                full_context += f"=== CHUNKS COM TAGS ESPEC√çFICAS - {empresa.upper()} ===\n\n"
                for chunk in tagged_chunks:
                    full_context += f"--- Tag '{chunk['tag_found']}' (Doc: {chunk['path']}) ---\n{chunk['text']}\n\n"
                    all_retrieved_docs.add(str(chunk['path']))
                full_context += f"=== FIM DOS CHUNKS COM TAGS - {empresa.upper()} ===\n\n"
    
    return full_context, [str(doc) for doc in all_retrieved_docs]

def get_final_unified_answer(query, context):
    """Gera resposta final usando API do Gemini"""
    try:
        api_key = st.secrets["GEMINI_API_KEY"]
    except:
        api_key = st.session_state.get('gemini_api_key')
        if not api_key:
            return "‚ùå API Key n√£o configurada para gerar resposta!"
    
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
    
    has_complete_8_4 = "=== SE√á√ÉO COMPLETA DO ITEM 8.4" in context
    
    if has_complete_8_4:
        structure_instruction = """
**ESTRUTURA PARA ITEM 8.4:**
Organize seguindo a estrutura oficial:
a) Termos e condi√ß√µes gerais
b) Data de aprova√ß√£o e √≥rg√£o respons√°vel  
c) N√∫mero m√°ximo de a√ß√µes abrangidas
d) N√∫mero m√°ximo de op√ß√µes a serem outorgadas
e) Condi√ß√µes de aquisi√ß√£o de a√ß√µes
f) Crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio
g) Crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio
h) Forma de liquida√ß√£o
i) Restri√ß√µes √† transfer√™ncia das a√ß√µes
j) Crit√©rios e eventos de suspens√£o, altera√ß√£o ou extin√ß√£o
k) Efeitos da sa√≠da do administrador
"""
    else:
        structure_instruction = "Organize a resposta de forma l√≥gica usando Markdown."
    
    prompt = f"""
Voc√™ √© um analista financeiro especializado em Formul√°rios de Refer√™ncia da CVM.

**PERGUNTA:** "{query}"

**CONTEXTO DOS DOCUMENTOS:**
{context}

{structure_instruction}

**INSTRU√á√ïES:**
1. Responda diretamente √† pergunta
2. **PRIORIZE** informa√ß√µes da SE√á√ÉO COMPLETA DO ITEM 8.4 quando dispon√≠vel
3. Seja detalhado, preciso e profissional
4. Transcreva dados importantes (valores, datas, percentuais)
5. Se informa√ß√£o n√£o dispon√≠vel: "Informa√ß√£o n√£o encontrada nas fontes"

**RELAT√ìRIO FINAL:**
"""
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}
    }
    headers = {'Content-Type': 'application/json'}
    
    response_data, error_message = safe_api_call(url, payload, headers, timeout=180)
    
    if error_message:
        return f"‚ùå Erro ao gerar resposta: {error_message}"
    
    try:
        return response_data['candidates'][0]['content']['parts'][0]['text'].strip()
    except:
        return "‚ùå Erro ao processar resposta final"

# --- INTERFACE STREAMLIT ---
def main():
    st.title("üöÄ Agente de An√°lise LTIP")
    st.markdown("**An√°lise de Formul√°rios de Refer√™ncia da CVM**")
    
    # SIDEBAR
    with st.sidebar:
        st.header("‚öôÔ∏è Configura√ß√µes")
        
        # API Key
        gemini_api_key = st.text_input("üîë API Key Gemini", type="password")
        if gemini_api_key:
            st.session_state['gemini_api_key'] = gemini_api_key
            st.success("‚úÖ API Key configurada!")
        
        # Info sobre arquivos
        st.subheader("üìÅ Status dos Arquivos")
        data_path = find_data_directory()
        if data_path:
            st.success(f"‚úÖ Pasta encontrada: {data_path}")
        else:
            st.error("‚ùå Pasta 'data' n√£o encontrada")
        
        if st.button("üîÑ Recarregar"):
            st.cache_resource.clear()
            st.rerun()
    
    # CARREGAMENTO
    if 'loaded_artifacts' not in st.session_state:
        if not st.session_state.get('gemini_api_key') and 'GEMINI_API_KEY' not in st.secrets:
            st.warning("‚ö†Ô∏è Configure a API Key do Gemini na barra lateral")
            return
        
        artifacts, model, company_catalog = load_all_artifacts()
        
        if artifacts is None:
            st.stop()
        
        st.session_state['loaded_artifacts'] = artifacts
        st.session_state['embedding_model'] = model
        st.session_state['company_catalog'] = company_catalog

    # INTERFACE PRINCIPAL
    loaded_artifacts = st.session_state['loaded_artifacts']
    company_catalog = st.session_state['company_catalog']
    
    # M√©tricas
    col1, col2 = st.columns(2)
    with col1:
        st.metric("üìä Categorias", len(loaded_artifacts))
    with col2:
        st.metric("üè¢ Empresas", len(company_catalog))
    
    # Empresas dispon√≠veis
    with st.expander("üè¢ Ver Empresas Dispon√≠veis"):
        for empresa in sorted(company_catalog):
            st.write(f"‚Ä¢ {empresa}")
    
    st.divider()
    
    # CONSULTA
    st.subheader("üí¨ Sua Pergunta")
    user_query = st.text_area("Digite aqui:", height=100)
    
    if st.button("üîç Analisar", type="primary", disabled=not user_query.strip()):
        try:
            # PLANEJAMENTO
            st.header("üìã Plano de An√°lise")
            with st.expander("Ver detalhes", expanded=True):
                plan_response = create_dynamic_analysis_plan(
                    user_query, company_catalog, list(loaded_artifacts.keys())
                )
                
                if plan_response['status'] != 'success':
                    st.error("‚ùå Erro no planejamento")
                    return
                
                plan = plan_response['plan']
                
                col1, col2 = st.columns(2)
                with col1:
                    st.write("**üè¢ Empresas:**")
                    for empresa in plan.get('empresas', []):
                        st.write(f"‚Ä¢ {empresa}")
                with col2:
                    st.write("**üìù T√≥picos:**")
                    for i, topico in enumerate(plan.get('topicos', [])[:5]):
                        st.write(f"{i+1}. {topico}")
                    if len(plan.get('topicos', [])) > 5:
                        st.write(f"... +{len(plan.get('topicos', [])) - 5} t√≥picos")
                
                if not plan.get("empresas"):
                    st.error("‚ùå Nenhuma empresa identificada")
                    return
            
            # DETEC√á√ÉO DE INTEN√á√ÉO
            query_intent = 'item_8_4_query' if any(term in user_query.lower() for term in 
                                                  ['8.4', '8-4', 'item 8.4', 'formul√°rio']) else 'general_query'
            st.info(f"**Estrat√©gia:** {query_intent}")
            
            # EXECU√á√ÉO
            st.header("üîç Recupera√ß√£o de Contexto")
            with st.expander("Ver busca", expanded=True):
                retrieved_context, sources = execute_dynamic_plan(
                    plan, query_intent, loaded_artifacts, st.session_state['embedding_model']
                )
                
                if not retrieved_context.strip():
                    st.warning("‚ö†Ô∏è Nenhuma informa√ß√£o relevante encontrada")
                    return
                
                st.success(f"‚úÖ Contexto de {len(set(sources))} documento(s)")
            
            # RESPOSTA
            st.header("üìÑ Resposta")
            with st.spinner("Gerando resposta..."):
                final_answer = get_final_unified_answer(user_query, retrieved_context)
            
            st.markdown(final_answer)
            
            # FONTES
            st.divider()
            with st.expander(f"üìö Fontes ({len(set(sources))})"):
                for source in sorted(set(sources)):
                    st.write(f"‚Ä¢ {source}")
                    
        except Exception as e:
            st.error(f"‚ùå Erro: {e}")

if __name__ == "__main__":
    main()
